{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a23288-ed16-47e8-b044-aa152b40ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4f966-931b-40df-80f6-48bd356813fd",
   "metadata": {},
   "source": [
    "## Getting the required Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d910b9bf-c164-4dc3-8ebb-d3057b305ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hardi\\anaconda3\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e293c843-d3de-4dc2-bb3b-064fb609b932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa5ee79-d6f9-4c2f-93e6-0b9177d1dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8978d9-795f-494c-ba14-40852a744a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfaa93b3-697a-4d3e-ac84-f3954e229399",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = \"\"\"Data science is the domain of study that deals with vast volumes of data using modern tools and techniques to find unseen patterns, derive meaningful information, and make business decisions. Data science uses complex machine learning algorithms to build predictive models.\n",
    "\n",
    "The data used for analysis can come from many different sources and presented in various formats.\n",
    "\n",
    "Now that you know what data science is, let’s see why data science is essential to today’s IT landscape.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9e00c-769a-448b-a2db-27df04874e8f",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3e73fc-5742-405e-b34b-3f1bde0f5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a8bcdc7-d40c-44ff-b406-4bcb8e4f3836",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_tokens = word_tokenize(DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9edbc56a-4142-4128-a42a-03d280e6f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b424b44-390d-497c-96f4-f3c57dc3e5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'data': 6, 'science': 4, '.': 4, 'is': 3, 'and': 3, 'to': 3, ',': 3, 'the': 2, 'of': 2, 'that': 2, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in DS_tokens:\n",
    "    fdist[word.lower()] += 1\n",
    "    \n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220c437e-b97f-4c3f-a534-fed23b950419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science is the domain of study that deals with vast volumes of data using modern tools and techniques to find unseen patterns, derive meaningful information, and make business decisions. Data science uses complex machine learning algorithms to build predictive models.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Blankline is used to separate the text by paragraphs if it has paragraphs\n",
    "\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "\n",
    "blnk = blankline_tokenize(DS)\n",
    "blnk[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978798dc-f994-47cb-857e-1c107e4e201e",
   "metadata": {},
   "source": [
    "#### Tokenize by n number of words\n",
    "\n",
    "Bigrams, trigrams, ngrams tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212499b9-6cce-4279-9266-6e53703c1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a574450a-0dea-4f3f-a16b-b08ce9b1eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_toks = list(nltk.bigrams(DS_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fae8c5c-ea40-4144-b687-16016c396c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science'),\n",
       " ('science', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'domain'),\n",
       " ('domain', 'of'),\n",
       " ('of', 'study'),\n",
       " ('study', 'that'),\n",
       " ('that', 'deals'),\n",
       " ('deals', 'with'),\n",
       " ('with', 'vast'),\n",
       " ('vast', 'volumes'),\n",
       " ('volumes', 'of'),\n",
       " ('of', 'data'),\n",
       " ('data', 'using'),\n",
       " ('using', 'modern'),\n",
       " ('modern', 'tools'),\n",
       " ('tools', 'and'),\n",
       " ('and', 'techniques'),\n",
       " ('techniques', 'to'),\n",
       " ('to', 'find'),\n",
       " ('find', 'unseen'),\n",
       " ('unseen', 'patterns'),\n",
       " ('patterns', ','),\n",
       " (',', 'derive'),\n",
       " ('derive', 'meaningful'),\n",
       " ('meaningful', 'information'),\n",
       " ('information', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'make'),\n",
       " ('make', 'business'),\n",
       " ('business', 'decisions'),\n",
       " ('decisions', '.'),\n",
       " ('.', 'Data'),\n",
       " ('Data', 'science'),\n",
       " ('science', 'uses'),\n",
       " ('uses', 'complex'),\n",
       " ('complex', 'machine'),\n",
       " ('machine', 'learning'),\n",
       " ('learning', 'algorithms'),\n",
       " ('algorithms', 'to'),\n",
       " ('to', 'build'),\n",
       " ('build', 'predictive'),\n",
       " ('predictive', 'models'),\n",
       " ('models', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'data'),\n",
       " ('data', 'used'),\n",
       " ('used', 'for'),\n",
       " ('for', 'analysis'),\n",
       " ('analysis', 'can'),\n",
       " ('can', 'come'),\n",
       " ('come', 'from'),\n",
       " ('from', 'many'),\n",
       " ('many', 'different'),\n",
       " ('different', 'sources'),\n",
       " ('sources', 'and'),\n",
       " ('and', 'presented'),\n",
       " ('presented', 'in'),\n",
       " ('in', 'various'),\n",
       " ('various', 'formats'),\n",
       " ('formats', '.'),\n",
       " ('.', 'Now'),\n",
       " ('Now', 'that'),\n",
       " ('that', 'you'),\n",
       " ('you', 'know'),\n",
       " ('know', 'what'),\n",
       " ('what', 'data'),\n",
       " ('data', 'science'),\n",
       " ('science', 'is'),\n",
       " ('is', ','),\n",
       " (',', 'let'),\n",
       " ('let', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'see'),\n",
       " ('see', 'why'),\n",
       " ('why', 'data'),\n",
       " ('data', 'science'),\n",
       " ('science', 'is'),\n",
       " ('is', 'essential'),\n",
       " ('essential', 'to'),\n",
       " ('to', 'today'),\n",
       " ('today', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'IT'),\n",
       " ('IT', 'landscape'),\n",
       " ('landscape', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42489154-3bb5-477b-932a-2efc8fb5b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly for trigrams\n",
    "\n",
    "trigrams_toks = list(nltk.trigrams(DS_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffb76b75-bd64-488a-b0ea-47d754b3c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_toks = list(nltk.ngrams(DS_tokens, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3043a47-056c-41a9-8636-29ad08ba62cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science', 'is', 'the', 'domain'),\n",
       " ('science', 'is', 'the', 'domain', 'of'),\n",
       " ('is', 'the', 'domain', 'of', 'study'),\n",
       " ('the', 'domain', 'of', 'study', 'that'),\n",
       " ('domain', 'of', 'study', 'that', 'deals'),\n",
       " ('of', 'study', 'that', 'deals', 'with'),\n",
       " ('study', 'that', 'deals', 'with', 'vast'),\n",
       " ('that', 'deals', 'with', 'vast', 'volumes'),\n",
       " ('deals', 'with', 'vast', 'volumes', 'of'),\n",
       " ('with', 'vast', 'volumes', 'of', 'data'),\n",
       " ('vast', 'volumes', 'of', 'data', 'using'),\n",
       " ('volumes', 'of', 'data', 'using', 'modern'),\n",
       " ('of', 'data', 'using', 'modern', 'tools'),\n",
       " ('data', 'using', 'modern', 'tools', 'and'),\n",
       " ('using', 'modern', 'tools', 'and', 'techniques'),\n",
       " ('modern', 'tools', 'and', 'techniques', 'to'),\n",
       " ('tools', 'and', 'techniques', 'to', 'find'),\n",
       " ('and', 'techniques', 'to', 'find', 'unseen'),\n",
       " ('techniques', 'to', 'find', 'unseen', 'patterns'),\n",
       " ('to', 'find', 'unseen', 'patterns', ','),\n",
       " ('find', 'unseen', 'patterns', ',', 'derive'),\n",
       " ('unseen', 'patterns', ',', 'derive', 'meaningful'),\n",
       " ('patterns', ',', 'derive', 'meaningful', 'information'),\n",
       " (',', 'derive', 'meaningful', 'information', ','),\n",
       " ('derive', 'meaningful', 'information', ',', 'and'),\n",
       " ('meaningful', 'information', ',', 'and', 'make'),\n",
       " ('information', ',', 'and', 'make', 'business'),\n",
       " (',', 'and', 'make', 'business', 'decisions'),\n",
       " ('and', 'make', 'business', 'decisions', '.'),\n",
       " ('make', 'business', 'decisions', '.', 'Data'),\n",
       " ('business', 'decisions', '.', 'Data', 'science'),\n",
       " ('decisions', '.', 'Data', 'science', 'uses'),\n",
       " ('.', 'Data', 'science', 'uses', 'complex'),\n",
       " ('Data', 'science', 'uses', 'complex', 'machine'),\n",
       " ('science', 'uses', 'complex', 'machine', 'learning'),\n",
       " ('uses', 'complex', 'machine', 'learning', 'algorithms'),\n",
       " ('complex', 'machine', 'learning', 'algorithms', 'to'),\n",
       " ('machine', 'learning', 'algorithms', 'to', 'build'),\n",
       " ('learning', 'algorithms', 'to', 'build', 'predictive'),\n",
       " ('algorithms', 'to', 'build', 'predictive', 'models'),\n",
       " ('to', 'build', 'predictive', 'models', '.'),\n",
       " ('build', 'predictive', 'models', '.', 'The'),\n",
       " ('predictive', 'models', '.', 'The', 'data'),\n",
       " ('models', '.', 'The', 'data', 'used'),\n",
       " ('.', 'The', 'data', 'used', 'for'),\n",
       " ('The', 'data', 'used', 'for', 'analysis'),\n",
       " ('data', 'used', 'for', 'analysis', 'can'),\n",
       " ('used', 'for', 'analysis', 'can', 'come'),\n",
       " ('for', 'analysis', 'can', 'come', 'from'),\n",
       " ('analysis', 'can', 'come', 'from', 'many'),\n",
       " ('can', 'come', 'from', 'many', 'different'),\n",
       " ('come', 'from', 'many', 'different', 'sources'),\n",
       " ('from', 'many', 'different', 'sources', 'and'),\n",
       " ('many', 'different', 'sources', 'and', 'presented'),\n",
       " ('different', 'sources', 'and', 'presented', 'in'),\n",
       " ('sources', 'and', 'presented', 'in', 'various'),\n",
       " ('and', 'presented', 'in', 'various', 'formats'),\n",
       " ('presented', 'in', 'various', 'formats', '.'),\n",
       " ('in', 'various', 'formats', '.', 'Now'),\n",
       " ('various', 'formats', '.', 'Now', 'that'),\n",
       " ('formats', '.', 'Now', 'that', 'you'),\n",
       " ('.', 'Now', 'that', 'you', 'know'),\n",
       " ('Now', 'that', 'you', 'know', 'what'),\n",
       " ('that', 'you', 'know', 'what', 'data'),\n",
       " ('you', 'know', 'what', 'data', 'science'),\n",
       " ('know', 'what', 'data', 'science', 'is'),\n",
       " ('what', 'data', 'science', 'is', ','),\n",
       " ('data', 'science', 'is', ',', 'let'),\n",
       " ('science', 'is', ',', 'let', '’'),\n",
       " ('is', ',', 'let', '’', 's'),\n",
       " (',', 'let', '’', 's', 'see'),\n",
       " ('let', '’', 's', 'see', 'why'),\n",
       " ('’', 's', 'see', 'why', 'data'),\n",
       " ('s', 'see', 'why', 'data', 'science'),\n",
       " ('see', 'why', 'data', 'science', 'is'),\n",
       " ('why', 'data', 'science', 'is', 'essential'),\n",
       " ('data', 'science', 'is', 'essential', 'to'),\n",
       " ('science', 'is', 'essential', 'to', 'today'),\n",
       " ('is', 'essential', 'to', 'today', '’'),\n",
       " ('essential', 'to', 'today', '’', 's'),\n",
       " ('to', 'today', '’', 's', 'IT'),\n",
       " ('today', '’', 's', 'IT', 'landscape'),\n",
       " ('’', 's', 'IT', 'landscape', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_toks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbb782-7f32-4c81-8a49-25b14351bad4",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is to find similar words which stems or originates from the same root word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "128fb259-0375-46c9-b146-ea88ce7b6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b144fa6-fa1a-4162-9759-0265b179652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f01d5c-843b-49b3-9290-2db56d3af88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "scienc\n",
      "is\n",
      "the\n",
      "domain\n",
      "of\n",
      "studi\n",
      "that\n",
      "deal\n",
      "with\n",
      "vast\n",
      "volum\n",
      "of\n",
      "data\n",
      "use\n",
      "modern\n",
      "tool\n",
      "and\n",
      "techniqu\n",
      "to\n",
      "find\n",
      "unseen\n",
      "pattern\n",
      ",\n",
      "deriv\n",
      "meaning\n",
      "inform\n",
      ",\n",
      "and\n",
      "make\n",
      "busi\n",
      "decis\n",
      ".\n",
      "data\n",
      "scienc\n",
      "use\n",
      "complex\n",
      "machin\n",
      "learn\n",
      "algorithm\n",
      "to\n",
      "build\n",
      "predict\n",
      "model\n",
      ".\n",
      "the\n",
      "data\n",
      "use\n",
      "for\n",
      "analysi\n",
      "can\n",
      "come\n",
      "from\n",
      "mani\n",
      "differ\n",
      "sourc\n",
      "and\n",
      "present\n",
      "in\n",
      "variou\n",
      "format\n",
      ".\n",
      "now\n",
      "that\n",
      "you\n",
      "know\n",
      "what\n",
      "data\n",
      "scienc\n",
      "is\n",
      ",\n",
      "let\n",
      "’\n",
      "s\n",
      "see\n",
      "whi\n",
      "data\n",
      "scienc\n",
      "is\n",
      "essenti\n",
      "to\n",
      "today\n",
      "’\n",
      "s\n",
      "it\n",
      "landscap\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in DS_tokens:\n",
    "    print(st.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6aeb2-afb8-40b0-b295-145f4f2c9c7f",
   "metadata": {},
   "source": [
    "## Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03b8559-2f26-4186-9fff-85e30a87f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df4f308-86b3-4724-a713-ca47ffdbf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bedd73e5-5c80-4570-b635-5ecaacb46c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noth'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"noth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d083302-445a-4d58-b1bb-a66575779e78",
   "metadata": {},
   "source": [
    "## Creating Bag of Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d132a3a1-dca5-4175-8fe1-c5779f60e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science is the domain of study that deals with vast volumes of data using modern tools and techniques to find unseen patterns, derive meaningful information, and make business decisions. Data science uses complex machine learning algorithms to build predictive models.\n",
      "\n",
      "The data used for analysis can come from many different sources and presented in various formats.\n",
      "\n",
      "Now that you know what data science is, let’s see why data science is essential to today’s IT landscape.\n"
     ]
    }
   ],
   "source": [
    "#lets get a paragraph with few setences\n",
    "\n",
    "print(DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8afb3f8-7f53-4aa0-a8e5-d465bde570c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we will create a list of sentences or a corpus\n",
    "\n",
    "#empty list of corpus\n",
    "corpus = []\n",
    "\n",
    "#from nltk import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05475dc9-4d6d-4ad8-995c-0721e20258a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data science is the domain of study that deals with vast volumes of data using modern tools and techniques to find unseen patterns, derive meaningful information, and make business decisions.', 'Data science uses complex machine learning algorithms to build predictive models.', 'The data used for analysis can come from many different sources and presented in various formats.', 'Now that you know what data science is, let’s see why data science is essential to today’s IT landscape.']\n"
     ]
    }
   ],
   "source": [
    "#now we have a list of sentences as below\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "483364f1-cbff-467c-8a3a-9b5f2abc4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we will try to remove any punctuation, remove stop words and create sentence with only required words\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "for sent in sentences:\n",
    "    sent = re.sub('[^a-zA-Z]',' ', sent)  \n",
    "    sent  = sent.lower()\n",
    "    sent = sent.split()\n",
    "    sent = [lemma.lemmatize(word) for word in sent if word not in set(stopwords.words(\"english\"))]\n",
    "    sent = \" \".join(sent)\n",
    "    corpus.append(sent)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "454d0d29-5157-4843-b1d1-5db176ecf645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science domain study deal vast volume data using modern tool technique find unseen pattern derive meaningful information make business decision',\n",
       " 'data science us complex machine learning algorithm build predictive model',\n",
       " 'data used analysis come many different source presented various format',\n",
       " 'know data science let see data science essential today landscape']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb067701-0712-4401-8791-cc096fe31d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will conver the words into a vector for each sentence \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#train data\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60125308-5fb6-41ea-a521-ed384245a14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93ee6f6e-31c2-4c20-b6d3-428c430439ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.22647466, 0.        ,\n",
       "        0.        , 0.23636775, 0.22647466, 0.22647466, 0.22647466,\n",
       "        0.        , 0.22647466, 0.        , 0.22647466, 0.        ,\n",
       "        0.22647466, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22647466, 0.        , 0.22647466, 0.        ,\n",
       "        0.22647466, 0.22647466, 0.        , 0.        , 0.14455584,\n",
       "        0.        , 0.        , 0.22647466, 0.22647466, 0.        ,\n",
       "        0.22647466, 0.22647466, 0.        , 0.        , 0.22647466,\n",
       "        0.        , 0.22647466, 0.22647466],\n",
       "       [0.33942742, 0.        , 0.33942742, 0.        , 0.        ,\n",
       "        0.33942742, 0.17712731, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.33942742, 0.        ,\n",
       "        0.33942742, 0.        , 0.        , 0.        , 0.33942742,\n",
       "        0.        , 0.        , 0.33942742, 0.        , 0.21665212,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.33942742, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.32840203, 0.        , 0.        , 0.32840203,\n",
       "        0.        , 0.1713738 , 0.        , 0.        , 0.        ,\n",
       "        0.32840203, 0.        , 0.        , 0.        , 0.32840203,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32840203, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.32840203, 0.        ,\n",
       "        0.        , 0.32840203, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.32840203, 0.        ,\n",
       "        0.32840203, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35345763, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.33866379, 0.        , 0.        ,\n",
       "        0.        , 0.33866379, 0.33866379, 0.        , 0.33866379,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.43232942,\n",
       "        0.33866379, 0.        , 0.        , 0.        , 0.33866379,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will try to use TfidfVectorizer to see the differene with Bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "X1 = tf.fit_transform(corpus).toarray()\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a907136-b189-4ff4-85b2-d995c77cc251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
